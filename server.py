#!/usr/bin/env python3
"""
æ–‡æ¡£é˜…è¯»å™¨ MCP æœåŠ¡å™¨
æ”¯æŒè¯»å– Word (.docx), PDF, TXT, RTF ç­‰æ ¼å¼çš„æ–‡æ¡£
"""

import asyncio
import json
import sys
import os
import base64
import re
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse

# MCP imports
from mcp.server import Server
from mcp.server.models import InitializationOptions
import mcp.server.stdio
import mcp.types as types

# æ–‡æ¡£å¤„ç†åº“
try:
    from docx import Document  # python-docx
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False

try:
    import PyPDF2
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

try:
    from striprtf.striprtf import rtf_to_text
    RTF_AVAILABLE = True
except ImportError:
    RTF_AVAILABLE = False

try:
    from PIL import Image
    PILLOW_AVAILABLE = True
except ImportError:
    PILLOW_AVAILABLE = False

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

try:
    import openpyxl
    import pandas as pd
    EXCEL_AVAILABLE = True
except ImportError:
    EXCEL_AVAILABLE = False

# å›¾åƒåˆ†æåº“
try:
    import cv2
    import numpy as np
    OPENCV_AVAILABLE = True
except ImportError:
    OPENCV_AVAILABLE = False

# å¯¼å…¥ç®€å•å›¾è¡¨é˜…è¯»å™¨
try:
    from simple_diagram_reader import SimpleDiagramReader, analyze_single_image
    SIMPLE_DIAGRAM_READER_AVAILABLE = True
except ImportError:
    SIMPLE_DIAGRAM_READER_AVAILABLE = False

# åˆ›å»ºæœåŠ¡å™¨å®ä¾‹
server = Server("document-reader")

@server.list_tools()
async def handle_list_tools() -> List[types.Tool]:
    """åˆ—å‡ºå¯ç”¨çš„å·¥å…·"""
    tools = [
        types.Tool(
            name="read_document",
            description="è¯»å–å„ç§æ ¼å¼çš„æ–‡æ¡£å†…å®¹ (Word .docx, PDF, Excel .xlsx/.xls, TXT, RTF)",
            inputSchema={
                "type": "object",
                "properties": {
                    "file_path": {
                        "type": "string",
                        "description": "æ–‡æ¡£æ–‡ä»¶çš„è·¯å¾„"
                    },
                    "page_range": {
                        "type": "string",
                        "description": "å¯é€‰ï¼šPDFé¡µé¢èŒƒå›´ï¼Œæ ¼å¼å¦‚ '1-5' æˆ– '1,3,5'",
                        "default": "all"
                    },
                    "sheet_name": {
                        "type": "string",
                        "description": "å¯é€‰ï¼šExcelå·¥ä½œè¡¨åç§°ï¼Œä¸æŒ‡å®šåˆ™è¯»å–æ‰€æœ‰å·¥ä½œè¡¨"
                    }
                },
                "required": ["file_path"]
            }
        ),
        types.Tool(
            name="get_document_info",
            description="è·å–æ–‡æ¡£çš„åŸºæœ¬ä¿¡æ¯ï¼ˆé¡µæ•°ã€æ ¼å¼ã€å¤§å°ç­‰ï¼‰",
            inputSchema={
                "type": "object",
                "properties": {
                    "file_path": {
                        "type": "string",
                        "description": "æ–‡æ¡£æ–‡ä»¶çš„è·¯å¾„"
                    }
                },
                "required": ["file_path"]
            }
        ),
        types.Tool(
            name="list_supported_formats",
            description="åˆ—å‡ºå½“å‰æ”¯æŒçš„æ–‡æ¡£æ ¼å¼",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        types.Tool(
            name="extract_document_media",
            description="æå–æ–‡æ¡£ä¸­çš„å›¾ç‰‡å’Œé“¾æ¥ä¿¡æ¯",
            inputSchema={
                "type": "object",
                "properties": {
                    "file_path": {
                        "type": "string",
                        "description": "æ–‡æ¡£æ–‡ä»¶çš„è·¯å¾„"
                    },
                    "extract_images": {
                        "type": "boolean",
                        "description": "æ˜¯å¦æå–å›¾ç‰‡ä¿¡æ¯",
                        "default": True
                    },
                    "extract_links": {
                        "type": "boolean",
                        "description": "æ˜¯å¦æå–é“¾æ¥ä¿¡æ¯",
                        "default": True
                    },
                    "save_images": {
                        "type": "boolean",
                        "description": "æ˜¯å¦ä¿å­˜æå–çš„å›¾ç‰‡åˆ°æœ¬åœ°",
                        "default": False
                    }
                },
                "required": ["file_path"]
            }
        ),
        types.Tool(
            name="read_document_with_media",
            description="è¯»å–æ–‡æ¡£å†…å®¹å¹¶åŒ…å«åª’ä½“å…ƒç´ ä¿¡æ¯ï¼ˆå›¾ç‰‡ã€é“¾æ¥ç­‰ï¼‰",
            inputSchema={
                "type": "object",
                "properties": {
                    "file_path": {
                        "type": "string",
                        "description": "æ–‡æ¡£æ–‡ä»¶çš„è·¯å¾„"
                    },
                    "page_range": {
                        "type": "string",
                        "description": "å¯é€‰ï¼šPDFé¡µé¢èŒƒå›´ï¼Œæ ¼å¼å¦‚ '1-5' æˆ– '1,3,5'",
                        "default": "all"
                    },
                    "include_media_info": {
                        "type": "boolean",
                        "description": "æ˜¯å¦åŒ…å«åª’ä½“å…ƒç´ ä¿¡æ¯",
                        "default": True
                    }
                },
                "required": ["file_path"]
            }
        ),

        types.Tool(
            name="read_diagram_content",
            description="ç›´æ¥è¯»å–å›¾è¡¨å†…å®¹ï¼Œæ— éœ€å¤æ‚OCRé…ç½®ï¼ŒåŸºäºå›¾åƒç»“æ„åˆ†æç†è§£å›¾è¡¨",
            inputSchema={
                "type": "object",
                "properties": {
                    "image_path": {
                        "type": "string",
                        "description": "å›¾ç‰‡æ–‡ä»¶çš„è·¯å¾„"
                    }
                },
                "required": ["image_path"]
            }
        )
    ]
    return tools

def read_docx_file(file_path: str) -> str:
    """è¯»å–Wordæ–‡æ¡£å†…å®¹"""
    if not DOCX_AVAILABLE:
        raise Exception("python-docx åº“æœªå®‰è£…ï¼Œæ— æ³•è¯»å– .docx æ–‡ä»¶")

    doc = Document(file_path)
    content = []

    for paragraph in doc.paragraphs:
        if paragraph.text.strip():
            content.append(paragraph.text)

    return "\n".join(content)

def read_pdf_file(file_path: str, page_range: str = "all") -> str:
    """è¯»å–PDFæ–‡æ¡£å†…å®¹"""
    if not PDF_AVAILABLE:
        raise Exception("PyPDF2 åº“æœªå®‰è£…ï¼Œæ— æ³•è¯»å– PDF æ–‡ä»¶")

    content = []

    with open(file_path, 'rb') as file:
        pdf_reader = PyPDF2.PdfReader(file)
        total_pages = len(pdf_reader.pages)

        # è§£æé¡µé¢èŒƒå›´
        if page_range == "all":
            pages_to_read = range(total_pages)
        else:
            pages_to_read = parse_page_range(page_range, total_pages)

        for page_num in pages_to_read:
            if 0 <= page_num < total_pages:
                page = pdf_reader.pages[page_num]
                content.append(f"=== ç¬¬ {page_num + 1} é¡µ ===\n{page.extract_text()}")

    return "\n\n".join(content)

def parse_page_range(page_range: str, total_pages: int) -> List[int]:
    """è§£æé¡µé¢èŒƒå›´å­—ç¬¦ä¸²"""
    pages = []

    for part in page_range.split(','):
        part = part.strip()
        if '-' in part:
            start, end = map(int, part.split('-'))
            pages.extend(range(start - 1, min(end, total_pages)))
        else:
            page_num = int(part) - 1
            if 0 <= page_num < total_pages:
                pages.append(page_num)

    return sorted(set(pages))

def read_txt_file(file_path: str) -> str:
    """è¯»å–æ–‡æœ¬æ–‡ä»¶å†…å®¹"""
    encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']

    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as file:
                return file.read()
        except UnicodeDecodeError:
            continue

    raise Exception(f"æ— æ³•ä½¿ç”¨å¸¸è§ç¼–ç è¯»å–æ–‡ä»¶: {file_path}")

def read_rtf_file(file_path: str) -> str:
    """è¯»å–RTFæ–‡ä»¶å†…å®¹"""
    if not RTF_AVAILABLE:
        raise Exception("striprtf åº“æœªå®‰è£…ï¼Œæ— æ³•è¯»å– RTF æ–‡ä»¶")

    with open(file_path, 'r', encoding='utf-8') as file:
        rtf_content = file.read()
        return rtf_to_text(rtf_content)

def read_excel_file(file_path: str, sheet_name: str = None) -> str:
    """è¯»å–Excelæ–‡ä»¶å†…å®¹"""
    if not EXCEL_AVAILABLE:
        raise Exception("openpyxl å’Œ pandas åº“æœªå®‰è£…ï¼Œæ— æ³•è¯»å– Excel æ–‡ä»¶")

    try:
        # ä½¿ç”¨pandasè¯»å–Excelæ–‡ä»¶
        if sheet_name:
            df = pd.read_excel(file_path, sheet_name=sheet_name)
            content = [f"=== å·¥ä½œè¡¨: {sheet_name} ===\n"]
        else:
            # è¯»å–æ‰€æœ‰å·¥ä½œè¡¨
            excel_file = pd.ExcelFile(file_path)
            content = []
            
            for sheet in excel_file.sheet_names:
                df = pd.read_excel(file_path, sheet_name=sheet)
                content.append(f"=== å·¥ä½œè¡¨: {sheet} ===")
                
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œä¿æŒè¡¨æ ¼ç»“æ„
                if not df.empty:
                    # å¤„ç†ç©ºå€¼
                    df_str = df.fillna('').astype(str)
                    content.append(df_str.to_string(index=False))
                else:
                    content.append("(ç©ºå·¥ä½œè¡¨)")
                content.append("")  # æ·»åŠ ç©ºè¡Œåˆ†éš”
            
            return "\n".join(content)
        
        # å•ä¸ªå·¥ä½œè¡¨çš„å¤„ç†
        if not df.empty:
            df_str = df.fillna('').astype(str)
            content.append(df_str.to_string(index=False))
        else:
            content.append("(ç©ºå·¥ä½œè¡¨)")
            
        return "\n".join(content)
        
    except Exception as e:
        raise Exception(f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {str(e)}")

def get_excel_info(file_path: str) -> Dict[str, Any]:
    """è·å–Excelæ–‡ä»¶ä¿¡æ¯"""
    if not EXCEL_AVAILABLE:
        raise Exception("openpyxl åº“æœªå®‰è£…ï¼Œæ— æ³•è·å– Excel æ–‡ä»¶ä¿¡æ¯")
    
    try:
        from openpyxl import load_workbook
        
        # è·å–æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(file_path)
        
        # åŠ è½½å·¥ä½œç°¿
        wb = load_workbook(file_path, read_only=True)
        
        info = {
            "format": "Excel (.xlsx/.xls)",
            "file_size": f"{file_size:,} å­—èŠ‚",
            "sheet_count": len(wb.sheetnames),
            "sheet_names": wb.sheetnames,
            "sheets_info": []
        }
        
        # è·å–æ¯ä¸ªå·¥ä½œè¡¨çš„ä¿¡æ¯
        for sheet_name in wb.sheetnames:
            ws = wb[sheet_name]
            sheet_info = {
                "name": sheet_name,
                "max_row": ws.max_row,
                "max_column": ws.max_column,
                "dimensions": f"{ws.max_row} è¡Œ Ã— {ws.max_column} åˆ—"
            }
            info["sheets_info"].append(sheet_info)
        
        wb.close()
        return info
        
    except Exception as e:
        raise Exception(f"è·å–Excelæ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")

def extract_excel_media(file_path: str, extract_images: bool = True, extract_links: bool = True, save_images: bool = False) -> Dict[str, Any]:
    """ä»Excelæ–‡ä»¶ä¸­æå–å›¾ç‰‡å’Œé“¾æ¥ä¿¡æ¯"""
    if not EXCEL_AVAILABLE:
        raise Exception("openpyxl åº“æœªå®‰è£…ï¼Œæ— æ³•å¤„ç† Excel æ–‡ä»¶")
    
    from openpyxl import load_workbook
    from openpyxl.drawing.image import Image as OpenpyxlImage
    import re
    
    result = {"images": [], "links": [], "summary": {}}
    
    try:
        # åŠ è½½å·¥ä½œç°¿
        wb = load_workbook(file_path, data_only=False)
        
        # æå–å›¾ç‰‡
        if extract_images:
            image_count = 0
            for sheet_name in wb.sheetnames:
                ws = wb[sheet_name]
                
                # æ£€æŸ¥å·¥ä½œè¡¨ä¸­çš„å›¾ç‰‡
                if hasattr(ws, '_images') and ws._images:
                    for img in ws._images:
                        try:
                            image_count += 1
                            image_info = {
                                "sheet": sheet_name,
                                "filename": f"excel_image_{image_count}.{img.format.lower()}",
                                "format": img.format,
                                "anchor": str(img.anchor) if hasattr(img, 'anchor') else "æœªçŸ¥ä½ç½®"
                            }
                            
                            # å¦‚æœéœ€è¦ä¿å­˜å›¾ç‰‡
                            if save_images and PILLOW_AVAILABLE:
                                try:
                                    # åˆ›å»ºä¿å­˜ç›®å½•
                                    save_dir = Path("extracted_images")
                                    save_dir.mkdir(exist_ok=True)
                                    
                                    # ä¿å­˜å›¾ç‰‡
                                    image_path = save_dir / image_info["filename"]
                                    with open(image_path, 'wb') as f:
                                        f.write(img._data())
                                    
                                    image_info["saved_path"] = str(image_path)
                                    image_info["file_size"] = len(img._data())
                                    
                                    # è·å–å›¾ç‰‡å°ºå¯¸
                                    try:
                                        from PIL import Image
                                        with Image.open(image_path) as pil_img:
                                            image_info["dimensions"] = pil_img.size
                                    except:
                                        pass
                                        
                                except Exception as e:
                                    image_info["save_error"] = str(e)
                            
                            result["images"].append(image_info)
                            
                        except Exception as e:
                            result["images"].append({
                                "sheet": sheet_name,
                                "error": f"å›¾ç‰‡å¤„ç†å¤±è´¥: {str(e)}"
                            })
        
        # æå–é“¾æ¥
        if extract_links:
            url_pattern = re.compile(r'https?://[^\s<>"{}|\\^`\[\]]+')
            
            for sheet_name in wb.sheetnames:
                ws = wb[sheet_name]
                
                # æ£€æŸ¥å•å…ƒæ ¼ä¸­çš„è¶…é“¾æ¥
                for row in ws.iter_rows():
                    for cell in row:
                        if cell.hyperlink:
                            try:
                                link_info = {
                                    "sheet": sheet_name,
                                    "cell": f"{cell.coordinate}",
                                    "url": cell.hyperlink.target,
                                    "display_text": str(cell.value) if cell.value else "",
                                    "type": "hyperlink"
                                }
                                
                                # éªŒè¯é“¾æ¥æœ‰æ•ˆæ€§
                                if REQUESTS_AVAILABLE and link_info["url"].startswith(('http://', 'https://')):
                                    try:
                                        import requests
                                        response = requests.head(link_info["url"], timeout=5, allow_redirects=True)
                                        link_info["status_code"] = response.status_code
                                        link_info["accessible"] = response.status_code < 400
                                    except:
                                        link_info["accessible"] = False
                                        link_info["status_code"] = "è¿æ¥å¤±è´¥"
                                
                                result["links"].append(link_info)
                                
                            except Exception as e:
                                result["links"].append({
                                    "sheet": sheet_name,
                                    "cell": f"{cell.coordinate}",
                                    "error": f"é“¾æ¥å¤„ç†å¤±è´¥: {str(e)}"
                                })
                        
                        # æ£€æŸ¥å•å…ƒæ ¼æ–‡æœ¬ä¸­çš„URL
                        elif cell.value and isinstance(cell.value, str):
                            urls = url_pattern.findall(str(cell.value))
                            for url in urls:
                                try:
                                    link_info = {
                                        "sheet": sheet_name,
                                        "cell": f"{cell.coordinate}",
                                        "url": url,
                                        "display_text": str(cell.value),
                                        "type": "text_url"
                                    }
                                    
                                    # éªŒè¯é“¾æ¥æœ‰æ•ˆæ€§
                                    if REQUESTS_AVAILABLE:
                                        try:
                                            import requests
                                            response = requests.head(url, timeout=5, allow_redirects=True)
                                            link_info["status_code"] = response.status_code
                                            link_info["accessible"] = response.status_code < 400
                                        except:
                                            link_info["accessible"] = False
                                            link_info["status_code"] = "è¿æ¥å¤±è´¥"
                                    
                                    result["links"].append(link_info)
                                    
                                except Exception as e:
                                    result["links"].append({
                                        "sheet": sheet_name,
                                        "cell": f"{cell.coordinate}",
                                        "url": url,
                                        "error": f"é“¾æ¥å¤„ç†å¤±è´¥: {str(e)}"
                                    })
        
        # ç”Ÿæˆæ‘˜è¦
        result["summary"] = {
            "image_count": len([img for img in result["images"] if "error" not in img]),
            "link_count": len([link for link in result["links"] if "error" not in link]),
            "image_errors": len([img for img in result["images"] if "error" in img]),
            "link_errors": len([link for link in result["links"] if "error" in link])
        }
        
        wb.close()
        return result
        
    except Exception as e:
        raise Exception(f"Excelåª’ä½“æå–å¤±è´¥: {str(e)}")

def read_excel_with_media(file_path: str, sheet_name: str = None) -> Tuple[str, Dict[str, Any]]:
    """è¯»å–Excelæ–‡æ¡£å†…å®¹å¹¶æå–åª’ä½“ä¿¡æ¯"""
    if not EXCEL_AVAILABLE:
        raise Exception("openpyxl å’Œ pandas åº“æœªå®‰è£…ï¼Œæ— æ³•è¯»å– Excel æ–‡ä»¶")
    
    # è¯»å–æ–‡æ¡£å†…å®¹
    content = read_excel_file(file_path, sheet_name)
    
    # æå–åª’ä½“ä¿¡æ¯
    try:
        media_data = extract_excel_media(file_path, extract_images=True, extract_links=True, save_images=False)
        media_info = {
            "images": media_data.get("images", []),
            "links": media_data.get("links", []),
            "summary": media_data.get("summary", {})
        }
        return content, media_info
    except Exception as e:
        # å¦‚æœåª’ä½“æå–å¤±è´¥ï¼Œè¿”å›ç©ºçš„åª’ä½“ä¿¡æ¯
        return content, {"images": [], "links": [], "summary": {"error": str(e)}}

def analyze_flowchart_image_from_bytes(image_bytes: bytes) -> dict:
    """åˆ†ææµç¨‹å›¾å›¾ç‰‡ï¼ŒåŸºäºOpenCVçš„åŸºç¡€ç»“æ„åˆ†æ"""
    result = {"text": "", "nodes": 0, "edges": 0}
    try:
        if not OPENCV_AVAILABLE:
            result["error"] = "OpenCVä¸å¯ç”¨"
            return result
            
        # è¯»å–å›¾ç‰‡
        import io
        file_bytes = np.asarray(bytearray(image_bytes), dtype=np.uint8)
        img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
        if img is None:
            result["error"] = "æ— æ³•è¯»å–å›¾ç‰‡"
            return result
            
        # ç°åº¦å¤„ç†
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        # è¾¹ç¼˜æ£€æµ‹
        edges = cv2.Canny(gray, 50, 150, apertureSize=3)
        
        # æ£€æµ‹åœ†å½¢èŠ‚ç‚¹
        circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, dp=1, minDist=40, param1=50, param2=30, minRadius=10, maxRadius=80)
        if circles is not None:
            result["nodes"] = len(circles[0])
            
        # æ£€æµ‹ç›´çº¿ï¼ˆè¿çº¿ï¼‰
        lines = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=80, minLineLength=40, maxLineGap=10)
        if lines is not None:
            result["edges"] = len(lines)
            
    except Exception as e:
        result["error"] = f"æµç¨‹å›¾è§£æå¤±è´¥: {str(e)}"
    return result

def format_simple_diagram_result(result: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–ç®€å•å›¾è¡¨åˆ†æç»“æœ"""
    if "error" in result:
        return f"åˆ†æå¤±è´¥: {result['error']}"
    
    result_text = "=== å›¾è¡¨å†…å®¹åˆ†æ ===\n\n"
    
    # æ–‡ä»¶ä¿¡æ¯
    if "file_info" in result:
        info = result["file_info"]
        result_text += f"ğŸ“Š æ–‡ä»¶ä¿¡æ¯:\n"
        result_text += f"- æ–‡ä»¶å: {info.get('filename', 'æœªçŸ¥')}\n"
        result_text += f"- å°ºå¯¸: {info.get('dimensions', 'æœªçŸ¥')}\n"
        result_text += f"- å¤§å°: {info.get('size', 'æœªçŸ¥')}\n\n"
    
    # å›¾è¡¨è§£é‡Š
    if "interpretation" in result:
        interp = result["interpretation"]
        result_text += f"ğŸ¯ å›¾è¡¨ç±»å‹: {interp.get('predicted_type', 'æœªçŸ¥')}\n"
        result_text += f"ğŸ“ˆ ç½®ä¿¡åº¦: {interp.get('confidence', 0):.1%}\n\n"
        result_text += f"ğŸ“ å†…å®¹æè¿°:\n{interp.get('content_description', 'æ— æè¿°')}\n\n"
        
        tech_elements = interp.get('technical_elements', [])
        if tech_elements:
            result_text += f"ğŸ”§ æŠ€æœ¯å…ƒç´ : {', '.join(tech_elements)}\n\n"
    
    # ç»“æ„åˆ†æ
    if "analysis" in result:
        analysis = result["analysis"]
        shapes = analysis.get("shapes", {})
        result_text += f"ğŸ—ï¸ ç»“æ„åˆ†æ:\n"
        result_text += f"- çŸ©å½¢æ¡†: {shapes.get('rectangles', 0)} ä¸ª\n"
        result_text += f"- åœ†å½¢: {shapes.get('circles', 0)} ä¸ª\n"
        result_text += f"- è¿æ¥çº¿: {shapes.get('lines', 0)} æ¡\n"
        result_text += f"- å¤æ‚åº¦è¯„åˆ†: {analysis.get('complexity', 0)}\n"
        result_text += f"- ä¸»è¦æ–¹å‘: {analysis.get('dominant_direction', 'æœªçŸ¥')}\n\n"
        
        layout = analysis.get("layout", {})
        if layout:
            result_text += f"ğŸ“ å¸ƒå±€ç‰¹å¾:\n"
            result_text += f"- å®½é«˜æ¯”: {layout.get('aspect_ratio', 0):.2f}\n"
            result_text += f"- æ–¹å‘: {layout.get('primary_orientation', 'æœªçŸ¥')}\n"
    
    return result_text



def extract_docx_media(file_path: str, extract_images: bool = True, extract_links: bool = True, save_images: bool = False) -> Dict[str, Any]:
    """ä»Wordæ–‡æ¡£ä¸­æå–å›¾ç‰‡å’Œé“¾æ¥ä¿¡æ¯"""
    if not DOCX_AVAILABLE:
        raise Exception("python-docx åº“æœªå®‰è£…ï¼Œæ— æ³•å¤„ç† .docx æ–‡ä»¶")

    from docx import Document
    from docx.document import Document as DocumentType
    from docx.oxml.table import CT_Tbl
    from docx.oxml.text.paragraph import CT_P
    from docx.table import _Cell, Table
    from docx.text.paragraph import Paragraph

    doc = Document(file_path)
    result = {"images": [], "links": [], "summary": {}}

    # æå–å›¾ç‰‡
    if extract_images and PILLOW_AVAILABLE:
        try:
            # ä»æ–‡æ¡£å…³ç³»ä¸­è·å–å›¾ç‰‡
            for rel in doc.part.rels.values():
                if "image" in rel.target_ref:
                    image_part = rel.target_part
                    image_data = image_part.blob

                    # è·å–å›¾ç‰‡ä¿¡æ¯
                    try:
                        from io import BytesIO
                        image = Image.open(BytesIO(image_data))

                        image_info = {
                            "filename": rel.target_ref.split('/')[-1],
                            "format": image.format,
                            "size": image.size,
                            "mode": image.mode,
                            "data_size": len(image_data)
                        }

                        # å¦‚æœéœ€è¦ä¿å­˜å›¾ç‰‡
                        if save_images:
                            output_dir = Path(file_path).parent / "extracted_images"
                            output_dir.mkdir(exist_ok=True)
                            image_path = output_dir / image_info["filename"]
                            with open(image_path, 'wb') as f:
                                f.write(image_data)
                            image_info["saved_path"] = str(image_path)

                        # æ–°å¢æµç¨‹å›¾å†…å®¹è§£æ
                        try:
                            flowchart_info = analyze_flowchart_image_from_bytes(image_data)
                            image_info["flowchart_analysis"] = flowchart_info
                        except Exception as e:
                            image_info["flowchart_analysis_error"] = str(e)

                        result["images"].append(image_info)
                    except Exception as e:
                        result["images"].append({
                            "filename": rel.target_ref.split('/')[-1],
                            "error": f"æ— æ³•å¤„ç†å›¾ç‰‡: {str(e)}"
                        })
        except Exception as e:
            result["images"] = [{"error": f"æå–å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}"}]

    # æå–é“¾æ¥
    if extract_links:
        try:
            # ä»æ®µè½ä¸­æå–è¶…é“¾æ¥
            for paragraph in doc.paragraphs:
                for run in paragraph.runs:
                    if run.element.tag.endswith('hyperlink') or any('hyperlink' in str(child.tag) for child in run.element):
                        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„é“¾æ¥æ£€æµ‹ï¼Œå®é™…å®ç°å¯èƒ½éœ€è¦æ›´å¤æ‚çš„XMLè§£æ
                        pass

            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»æ–‡æœ¬ä¸­æå–URL
            full_text = "\n".join([p.text for p in doc.paragraphs])
            url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
            urls = re.findall(url_pattern, full_text)

            for url in set(urls):  # å»é‡
                link_info = {
                    "url": url,
                    "domain": urlparse(url).netloc,
                    "scheme": urlparse(url).scheme
                }

                # å¦‚æœæœ‰requestsåº“ï¼Œæ£€æŸ¥é“¾æ¥æœ‰æ•ˆæ€§
                if REQUESTS_AVAILABLE:
                    try:
                        response = requests.head(url, timeout=5, allow_redirects=True)
                        link_info["status_code"] = response.status_code
                        link_info["accessible"] = response.status_code < 400
                    except:
                        link_info["accessible"] = False
                        link_info["status_code"] = None

                result["links"].append(link_info)

        except Exception as e:
            result["links"] = [{"error": f"æå–é“¾æ¥æ—¶å‡ºé”™: {str(e)}"}]

    # ç”Ÿæˆæ‘˜è¦
    result["summary"] = {
        "total_images": len([img for img in result["images"] if "error" not in img]),
        "total_links": len([link for link in result["links"] if "error" not in link]),
        "images_with_errors": len([img for img in result["images"] if "error" in img]),
        "links_with_errors": len([link for link in result["links"] if "error" in link])
    }

    return result

def extract_pdf_links(file_path: str) -> List[Dict[str, Any]]:
    """ä»PDFæ–‡æ¡£ä¸­æå–é“¾æ¥ä¿¡æ¯"""
    if not PDF_AVAILABLE:
        raise Exception("PyPDF2 åº“æœªå®‰è£…ï¼Œæ— æ³•å¤„ç† PDF æ–‡ä»¶")

    links = []
    try:
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)

            for page_num, page in enumerate(pdf_reader.pages):
                # æå–é¡µé¢æ–‡æœ¬ä¸­çš„URL
                text = page.extract_text()
                url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
                urls = re.findall(url_pattern, text)

                for url in urls:
                    link_info = {
                        "url": url,
                        "page": page_num + 1,
                        "domain": urlparse(url).netloc,
                        "scheme": urlparse(url).scheme
                    }

                    # æ£€æŸ¥é“¾æ¥æœ‰æ•ˆæ€§
                    if REQUESTS_AVAILABLE:
                        try:
                            response = requests.head(url, timeout=5, allow_redirects=True)
                            link_info["status_code"] = response.status_code
                            link_info["accessible"] = response.status_code < 400
                        except:
                            link_info["accessible"] = False
                            link_info["status_code"] = None

                    links.append(link_info)

    except Exception as e:
        return [{"error": f"æå–PDFé“¾æ¥æ—¶å‡ºé”™: {str(e)}"}]

    return links

def read_docx_with_media(file_path: str) -> Tuple[str, Dict[str, Any]]:
    """è¯»å–Wordæ–‡æ¡£å†…å®¹å¹¶æå–åª’ä½“ä¿¡æ¯"""
    if not DOCX_AVAILABLE:
        raise Exception("python-docx åº“æœªå®‰è£…ï¼Œæ— æ³•è¯»å– .docx æ–‡ä»¶")

    doc = Document(file_path)
    content = []
    media_info = {"images": [], "links": []}

    # æå–æ–‡æœ¬å†…å®¹
    for paragraph in doc.paragraphs:
        if paragraph.text.strip():
            content.append(paragraph.text)

    # æå–åª’ä½“ä¿¡æ¯
    try:
        media_data = extract_docx_media(file_path, extract_images=True, extract_links=True, save_images=False)
        media_info = {
            "images": media_data.get("images", []),
            "links": media_data.get("links", []),
            "summary": media_data.get("summary", {})
        }
    except Exception as e:
        media_info["error"] = f"æå–åª’ä½“ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}"

    return "\n".join(content), media_info

def get_file_info(file_path: str) -> Dict[str, Any]:
    """è·å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯"""
    path = Path(file_path)

    if not path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    info = {
        "æ–‡ä»¶å": path.name,
        "æ–‡ä»¶å¤§å°": f"{path.stat().st_size / 1024:.2f} KB",
        "æ–‡ä»¶æ ¼å¼": path.suffix.lower(),
        "ç»å¯¹è·¯å¾„": str(path.absolute())
    }

    # æ ¹æ®æ–‡ä»¶ç±»å‹æ·»åŠ ç‰¹å®šä¿¡æ¯
    if path.suffix.lower() == '.docx' and DOCX_AVAILABLE:
        try:
            doc = Document(file_path)
            info["æ®µè½æ•°"] = len(doc.paragraphs)
        except:
            pass

    elif path.suffix.lower() == '.pdf' and PDF_AVAILABLE:
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                info["é¡µæ•°"] = len(pdf_reader.pages)
        except:
            pass
    
    elif path.suffix.lower() in ['.xlsx', '.xls'] and EXCEL_AVAILABLE:
        try:
            excel_info = get_excel_info(file_path)
            info.update(excel_info)
        except:
            pass

    return info

@server.call_tool()
async def handle_call_tool(
    name: str, arguments: Dict[str, Any]
) -> List[types.TextContent]:
    """å¤„ç†å·¥å…·è°ƒç”¨"""

    if name == "read_document":
        file_path = arguments.get("file_path")
        page_range = arguments.get("page_range", "all")

        if not file_path:
            return [types.TextContent(
                type="text",
                text="é”™è¯¯ï¼šå¿…é¡»æä¾›æ–‡ä»¶è·¯å¾„"
            )]

        try:
            path = Path(file_path)

            if not path.exists():
                return [types.TextContent(
                    type="text",
                    text=f"é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ - {file_path}"
                )]

            file_ext = path.suffix.lower()

            if file_ext == '.docx':
                content = read_docx_file(file_path)
            elif file_ext == '.pdf':
                content = read_pdf_file(file_path, page_range)
            elif file_ext in ['.xlsx', '.xls']:
                sheet_name = arguments.get("sheet_name")
                content = read_excel_file(file_path, sheet_name)
            elif file_ext in ['.txt', '.md', '.py', '.js', '.html', '.css']:
                content = read_txt_file(file_path)
            elif file_ext == '.rtf':
                content = read_rtf_file(file_path)
            else:
                return [types.TextContent(
                    type="text",
                    text=f"é”™è¯¯ï¼šä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ - {file_ext}"
                )]

            return [types.TextContent(
                type="text",
                text=f"æ–‡æ¡£å†…å®¹ ({path.name}):\n\n{content}"
            )]

        except Exception as e:
            return [types.TextContent(
                type="text",
                text=f"è¯»å–æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}"
            )]

    elif name == "get_document_info":
        file_path = arguments.get("file_path")

        if not file_path:
            return [types.TextContent(
                type="text",
                text="é”™è¯¯ï¼šå¿…é¡»æä¾›æ–‡ä»¶è·¯å¾„"
            )]

        try:
            info = get_file_info(file_path)
            info_text = "\n".join([f"{k}: {v}" for k, v in info.items()])

            return [types.TextContent(
                type="text",
                text=f"æ–‡æ¡£ä¿¡æ¯:\n{info_text}"
            )]

        except Exception as e:
            return [types.TextContent(
                type="text",
                text=f"è·å–æ–‡æ¡£ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}"
            )]

    elif name == "list_supported_formats":
        formats = {
            ".docx": "Wordæ–‡æ¡£" + (" âœ“" if DOCX_AVAILABLE else " âœ— (éœ€è¦ python-docx)"),
            ".pdf": "PDFæ–‡æ¡£" + (" âœ“" if PDF_AVAILABLE else " âœ— (éœ€è¦ PyPDF2)"),
            ".xlsx/.xls": "Excelæ–‡æ¡£" + (" âœ“" if EXCEL_AVAILABLE else " âœ— (éœ€è¦ openpyxl, pandas)"),
            ".txt": "çº¯æ–‡æœ¬æ–‡ä»¶ âœ“",
            ".md": "Markdownæ–‡ä»¶ âœ“",
            ".rtf": "RTFæ–‡æ¡£" + (" âœ“" if RTF_AVAILABLE else " âœ— (éœ€è¦ striprtf)"),
            ".py/.js/.html/.css": "ä»£ç æ–‡ä»¶ âœ“"
        }

        # æ·»åŠ åª’ä½“å¤„ç†èƒ½åŠ›ä¿¡æ¯
        media_support = {
            "å›¾ç‰‡å¤„ç†": " âœ“" if PILLOW_AVAILABLE else " âœ— (éœ€è¦ Pillow)",
            "é“¾æ¥éªŒè¯": " âœ“" if REQUESTS_AVAILABLE else " âœ— (éœ€è¦ requests)"
        }

        format_text = "\n".join([f"{ext}: {desc}" for ext, desc in formats.items()])
        media_text = "\n".join([f"{feature}: {status}" for feature, status in media_support.items()])

        return [types.TextContent(
            type="text",
            text=f"æ”¯æŒçš„æ–‡æ¡£æ ¼å¼:\n{format_text}\n\nåª’ä½“å¤„ç†èƒ½åŠ›:\n{media_text}"
        )]

    elif name == "extract_document_media":
        file_path = arguments.get("file_path")
        extract_images = arguments.get("extract_images", True)
        extract_links = arguments.get("extract_links", True)
        save_images = arguments.get("save_images", False)

        if not file_path:
            return [types.TextContent(
                type="text",
                text="é”™è¯¯ï¼šå¿…é¡»æä¾›æ–‡ä»¶è·¯å¾„"
            )]

        try:
            path = Path(file_path)

            if not path.exists():
                return [types.TextContent(
                    type="text",
                    text=f"é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ - {file_path}"
                )]

            file_ext = path.suffix.lower()

            if file_ext == '.docx':
                media_data = extract_docx_media(file_path, extract_images, extract_links, save_images)

                result_text = f"æ–‡æ¡£åª’ä½“ä¿¡æ¯ ({path.name}):\n\n"

                # æ‘˜è¦ä¿¡æ¯
                summary = media_data.get("summary", {})
                result_text += f"æ‘˜è¦:\n"
                result_text += f"- å›¾ç‰‡æ€»æ•°: {summary.get('total_images', 0)}\n"
                result_text += f"- é“¾æ¥æ€»æ•°: {summary.get('total_links', 0)}\n"
                result_text += f"- å›¾ç‰‡å¤„ç†é”™è¯¯: {summary.get('images_with_errors', 0)}\n"
                result_text += f"- é“¾æ¥å¤„ç†é”™è¯¯: {summary.get('links_with_errors', 0)}\n\n"

                # å›¾ç‰‡ä¿¡æ¯
                if extract_images and media_data.get("images"):
                    result_text += "å›¾ç‰‡ä¿¡æ¯:\n"
                    for i, img in enumerate(media_data["images"], 1):
                        if "error" in img:
                            result_text += f"{i}. é”™è¯¯: {img['error']}\n"
                        else:
                            result_text += f"{i}. {img.get('filename', 'æœªçŸ¥æ–‡ä»¶å')}\n"
                            result_text += f"   - æ ¼å¼: {img.get('format', 'æœªçŸ¥')}\n"
                            result_text += f"   - å°ºå¯¸: {img.get('size', 'æœªçŸ¥')}\n"
                            result_text += f"   - å¤§å°: {img.get('data_size', 0)} å­—èŠ‚\n"
                            if "saved_path" in img:
                                result_text += f"   - ä¿å­˜è·¯å¾„: {img['saved_path']}\n"
                            if "flowchart_analysis" in img:
                                flowchart = img["flowchart_analysis"]
                                result_text += f"   - æµç¨‹å›¾åˆ†æ:\n"
                                result_text += f"     - æ–‡æœ¬: {flowchart.get('text', 'æ— ')}\n"
                                result_text += f"     - èŠ‚ç‚¹æ•°: {flowchart.get('nodes', 0)}\n"
                                result_text += f"     - è¿çº¿æ•°: {flowchart.get('edges', 0)}\n"
                            if "flowchart_analysis_error" in img:
                                result_text += f"   - æµç¨‹å›¾åˆ†æé”™è¯¯: {img['flowchart_analysis_error']}\n"
                    result_text += "\n"

                # é“¾æ¥ä¿¡æ¯
                if extract_links and media_data.get("links"):
                    result_text += "é“¾æ¥ä¿¡æ¯:\n"
                    for i, link in enumerate(media_data["links"], 1):
                        if "error" in link:
                            result_text += f"{i}. é”™è¯¯: {link['error']}\n"
                        else:
                            result_text += f"{i}. {link['url']}\n"
                            result_text += f"   - åŸŸå: {link.get('domain', 'æœªçŸ¥')}\n"
                            result_text += f"   - åè®®: {link.get('scheme', 'æœªçŸ¥')}\n"
                            if "accessible" in link:
                                status = "å¯è®¿é—®" if link["accessible"] else "ä¸å¯è®¿é—®"
                                result_text += f"   - çŠ¶æ€: {status}"
                                if link.get("status_code"):
                                    result_text += f" (HTTP {link['status_code']})"
                                result_text += "\n"

                return [types.TextContent(
                    type="text",
                    text=result_text
                )]

            elif file_ext in ['.xlsx', '.xls']:
                media_data = extract_excel_media(file_path, extract_images, extract_links, save_images)

                result_text = f"Excelåª’ä½“ä¿¡æ¯ ({path.name}):\n\n"

                # æ‘˜è¦ä¿¡æ¯
                summary = media_data.get("summary", {})
                result_text += f"æ‘˜è¦:\n"
                result_text += f"- å›¾ç‰‡æ€»æ•°: {summary.get('image_count', 0)}\n"
                result_text += f"- é“¾æ¥æ€»æ•°: {summary.get('link_count', 0)}\n"
                result_text += f"- å›¾ç‰‡å¤„ç†é”™è¯¯: {summary.get('image_errors', 0)}\n"
                result_text += f"- é“¾æ¥å¤„ç†é”™è¯¯: {summary.get('link_errors', 0)}\n\n"

                # å›¾ç‰‡ä¿¡æ¯
                if extract_images and media_data.get("images"):
                    result_text += "å›¾ç‰‡ä¿¡æ¯:\n"
                    for i, img in enumerate(media_data["images"], 1):
                        if "error" in img:
                            result_text += f"{i}. é”™è¯¯: {img['error']}\n"
                        else:
                            result_text += f"{i}. {img.get('filename', 'æœªçŸ¥æ–‡ä»¶å')}\n"
                            result_text += f"   - å·¥ä½œè¡¨: {img.get('sheet', 'æœªçŸ¥')}\n"
                            result_text += f"   - æ ¼å¼: {img.get('format', 'æœªçŸ¥')}\n"
                            result_text += f"   - ä½ç½®: {img.get('anchor', 'æœªçŸ¥')}\n"
                            if "dimensions" in img:
                                result_text += f"   - å°ºå¯¸: {img['dimensions']}\n"
                            if "file_size" in img:
                                result_text += f"   - å¤§å°: {img['file_size']} å­—èŠ‚\n"
                            if "saved_path" in img:
                                result_text += f"   - ä¿å­˜è·¯å¾„: {img['saved_path']}\n"
                    result_text += "\n"

                # é“¾æ¥ä¿¡æ¯
                if extract_links and media_data.get("links"):
                    result_text += "é“¾æ¥ä¿¡æ¯:\n"
                    for i, link in enumerate(media_data["links"], 1):
                        if "error" in link:
                            result_text += f"{i}. é”™è¯¯: {link['error']}\n"
                        else:
                            result_text += f"{i}. {link['url']}\n"
                            result_text += f"   - å·¥ä½œè¡¨: {link.get('sheet', 'æœªçŸ¥')}\n"
                            result_text += f"   - å•å…ƒæ ¼: {link.get('cell', 'æœªçŸ¥')}\n"
                            result_text += f"   - ç±»å‹: {link.get('type', 'æœªçŸ¥')}\n"
                            if link.get('display_text'):
                                result_text += f"   - æ˜¾ç¤ºæ–‡æœ¬: {link['display_text'][:50]}{'...' if len(link['display_text']) > 50 else ''}\n"
                            if "accessible" in link:
                                status = "å¯è®¿é—®" if link["accessible"] else "ä¸å¯è®¿é—®"
                                result_text += f"   - çŠ¶æ€: {status}"
                                if link.get("status_code"):
                                    result_text += f" (HTTP {link['status_code']})"
                                result_text += "\n"

                return [types.TextContent(
                    type="text",
                    text=result_text
                )]

            elif file_ext == '.pdf':
                if extract_links:
                    links = extract_pdf_links(file_path)
                    result_text = f"PDFé“¾æ¥ä¿¡æ¯ ({path.name}):\n\n"

                    if links and not any("error" in link for link in links):
                        result_text += f"æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥:\n\n"
                        for i, link in enumerate(links, 1):
                            result_text += f"{i}. {link['url']} (ç¬¬{link['page']}é¡µ)\n"
                            result_text += f"   - åŸŸå: {link.get('domain', 'æœªçŸ¥')}\n"
                            if "accessible" in link:
                                status = "å¯è®¿é—®" if link["accessible"] else "ä¸å¯è®¿é—®"
                                result_text += f"   - çŠ¶æ€: {status}"
                                if link.get("status_code"):
                                    result_text += f" (HTTP {link['status_code']})"
                                result_text += "\n"
                    else:
                        result_text += "æœªæ‰¾åˆ°é“¾æ¥æˆ–å¤„ç†æ—¶å‡ºé”™\n"
                        for link in links:
                            if "error" in link:
                                result_text += f"é”™è¯¯: {link['error']}\n"

                    return [types.TextContent(
                        type="text",
                        text=result_text
                    )]
                else:
                    return [types.TextContent(
                        type="text",
                        text="PDFæ–‡æ¡£ç›®å‰åªæ”¯æŒé“¾æ¥æå–ï¼Œè¯·å¯ç”¨ extract_links å‚æ•°"
                    )]
            else:
                return [types.TextContent(
                    type="text",
                    text=f"é”™è¯¯ï¼šæ–‡ä»¶æ ¼å¼ {file_ext} æš‚ä¸æ”¯æŒåª’ä½“æå–åŠŸèƒ½"
                )]

        except Exception as e:
            return [types.TextContent(
                type="text",
                text=f"æå–åª’ä½“ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}"
            )]



    elif name == "read_diagram_content":
        image_path = arguments.get("image_path")

        if not image_path:
            return [types.TextContent(
                type="text",
                text="é”™è¯¯ï¼šå¿…é¡»æä¾›å›¾ç‰‡è·¯å¾„"
            )]

        try:
            path = Path(image_path)

            if not path.exists():
                return [types.TextContent(
                    type="text",
                    text=f"é”™è¯¯ï¼šå›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨ - {image_path}"
                )]

            # æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡æ–‡ä»¶
            image_extensions = ['.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.gif', '.emf', '.wmf']
            if path.suffix.lower() not in image_extensions:
                return [types.TextContent(
                    type="text",
                    text=f"é”™è¯¯ï¼šä¸æ”¯æŒçš„å›¾ç‰‡æ ¼å¼ - {path.suffix}"
                )]

            # ä½¿ç”¨ç®€å•å›¾è¡¨é˜…è¯»å™¨
            if SIMPLE_DIAGRAM_READER_AVAILABLE:
                result = analyze_single_image(image_path)
                
                if "error" in result:
                    return [types.TextContent(
                        type="text",
                        text=f"å›¾è¡¨åˆ†æå¤±è´¥: {result['error']}"
                    )]
                
                # æ ¼å¼åŒ–ç»“æœ
                formatted_result = format_simple_diagram_result(result)
                
                return [types.TextContent(
                    type="text",
                    text=formatted_result
                )]
            else:
                return [types.TextContent(
                    type="text",
                    text="é”™è¯¯ï¼šç®€å•å›¾è¡¨é˜…è¯»å™¨ä¸å¯ç”¨"
                )]

        except Exception as e:
            return [types.TextContent(
                type="text",
                text=f"è¯»å–å›¾è¡¨å†…å®¹æ—¶å‡ºé”™: {str(e)}"
            )]

    elif name == "read_document_with_media":
        file_path = arguments.get("file_path")
        page_range = arguments.get("page_range", "all")
        include_media_info = arguments.get("include_media_info", True)

        if not file_path:
            return [types.TextContent(
                type="text",
                text="é”™è¯¯ï¼šå¿…é¡»æä¾›æ–‡ä»¶è·¯å¾„"
            )]

        try:
            path = Path(file_path)

            if not path.exists():
                return [types.TextContent(
                    type="text",
                    text=f"é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ - {file_path}"
                )]

            file_ext = path.suffix.lower()

            if file_ext == '.docx':
                if include_media_info:
                    content, media_info = read_docx_with_media(file_path)

                    result_text = f"æ–‡æ¡£å†…å®¹ ({path.name}):\n\n{content}\n\n"

                    # æ·»åŠ åª’ä½“ä¿¡æ¯
                    if "error" not in media_info:
                        summary = media_info.get("summary", {})
                        result_text += "=== åª’ä½“ä¿¡æ¯ ===\n"
                        result_text += f"å›¾ç‰‡æ•°é‡: {summary.get('total_images', 0)}\n"
                        result_text += f"é“¾æ¥æ•°é‡: {summary.get('total_links', 0)}\n"

                        if media_info.get("images"):
                            result_text += "\nå›¾ç‰‡åˆ—è¡¨:\n"
                            for i, img in enumerate(media_info["images"], 1):
                                if "error" not in img:
                                    result_text += f"{i}. {img.get('filename', 'æœªçŸ¥')} ({img.get('format', 'æœªçŸ¥')}, {img.get('size', 'æœªçŸ¥')})\n"

                        if media_info.get("links"):
                            result_text += "\né“¾æ¥åˆ—è¡¨:\n"
                            for i, link in enumerate(media_info["links"], 1):
                                if "error" not in link:
                                    result_text += f"{i}. {link['url']}\n"
                    else:
                        result_text += f"åª’ä½“ä¿¡æ¯æå–é”™è¯¯: {media_info['error']}\n"

                    return [types.TextContent(
                        type="text",
                        text=result_text
                    )]
                else:
                    content = read_docx_file(file_path)
                    return [types.TextContent(
                        type="text",
                        text=f"æ–‡æ¡£å†…å®¹ ({path.name}):\n\n{content}"
                    )]

            elif file_ext == '.pdf':
                content = read_pdf_file(file_path, page_range)

                if include_media_info:
                    # ä¸ºPDFæ·»åŠ é“¾æ¥ä¿¡æ¯
                    try:
                        links = extract_pdf_links(file_path)
                        result_text = f"æ–‡æ¡£å†…å®¹ ({path.name}):\n\n{content}\n\n"

                        if links and not any("error" in link for link in links):
                            result_text += "=== é“¾æ¥ä¿¡æ¯ ===\n"
                            result_text += f"æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥:\n"
                            for i, link in enumerate(links, 1):
                                result_text += f"{i}. {link['url']} (ç¬¬{link['page']}é¡µ)\n"

                        return [types.TextContent(
                            type="text",
                            text=result_text
                        )]
                    except:
                        # å¦‚æœé“¾æ¥æå–å¤±è´¥ï¼Œåªè¿”å›å†…å®¹
                        return [types.TextContent(
                            type="text",
                            text=f"æ–‡æ¡£å†…å®¹ ({path.name}):\n\n{content}"
                        )]
                else:
                    return [types.TextContent(
                        type="text",
                        text=f"æ–‡æ¡£å†…å®¹ ({path.name}):\n\n{content}"
                    )]

            elif file_ext in ['.xlsx', '.xls']:
                sheet_name = arguments.get("sheet_name")
                
                if include_media_info:
                    content, media_info = read_excel_with_media(file_path, sheet_name)

                    result_text = f"æ–‡æ¡£å†…å®¹ ({path.name}):\n\n{content}\n\n"

                    # æ·»åŠ åª’ä½“ä¿¡æ¯
                    if "error" not in media_info.get("summary", {}):
                        summary = media_info.get("summary", {})
                        result_text += "=== åª’ä½“ä¿¡æ¯ ===\n"
                        result_text += f"å›¾ç‰‡æ•°é‡: {summary.get('image_count', 0)}\n"
                        result_text += f"é“¾æ¥æ•°é‡: {summary.get('link_count', 0)}\n"

                        if media_info.get("images"):
                            result_text += "\nå›¾ç‰‡åˆ—è¡¨:\n"
                            for i, img in enumerate(media_info["images"], 1):
                                if "error" not in img:
                                    result_text += f"{i}. {img.get('filename', 'æœªçŸ¥')} ({img.get('format', 'æœªçŸ¥')}, å·¥ä½œè¡¨: {img.get('sheet', 'æœªçŸ¥')})\n"

                        if media_info.get("links"):
                            result_text += "\né“¾æ¥åˆ—è¡¨:\n"
                            for i, link in enumerate(media_info["links"], 1):
                                if "error" not in link:
                                    result_text += f"{i}. {link['url']} (å·¥ä½œè¡¨: {link.get('sheet', 'æœªçŸ¥')}, å•å…ƒæ ¼: {link.get('cell', 'æœªçŸ¥')})\n"
                    else:
                        result_text += f"åª’ä½“ä¿¡æ¯æå–é”™è¯¯: {media_info.get('summary', {}).get('error', 'æœªçŸ¥é”™è¯¯')}\n"

                    return [types.TextContent(
                        type="text",
                        text=result_text
                    )]
                else:
                    content = read_excel_file(file_path, sheet_name)
                    return [types.TextContent(
                        type="text",
                        text=f"æ–‡æ¡£å†…å®¹ ({path.name}):\n\n{content}"
                    )]

            elif file_ext in ['.txt', '.md', '.py', '.js', '.html', '.css']:
                content = read_txt_file(file_path)

                if include_media_info:
                    # ä»æ–‡æœ¬æ–‡ä»¶ä¸­æå–é“¾æ¥
                    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
                    urls = re.findall(url_pattern, content)

                    result_text = f"æ–‡æ¡£å†…å®¹ ({path.name}):\n\n{content}\n\n"

                    if urls:
                        result_text += "=== é“¾æ¥ä¿¡æ¯ ===\n"
                        result_text += f"æ‰¾åˆ° {len(set(urls))} ä¸ªé“¾æ¥:\n"
                        for i, url in enumerate(set(urls), 1):
                            result_text += f"{i}. {url}\n"

                    return [types.TextContent(
                        type="text",
                        text=result_text
                    )]
                else:
                    return [types.TextContent(
                        type="text",
                        text=f"æ–‡æ¡£å†…å®¹ ({path.name}):\n\n{content}"
                    )]

            elif file_ext == '.rtf':
                content = read_rtf_file(file_path)
                return [types.TextContent(
                    type="text",
                    text=f"æ–‡æ¡£å†…å®¹ ({path.name}):\n\n{content}"
                )]
            else:
                return [types.TextContent(
                    type="text",
                    text=f"é”™è¯¯ï¼šä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ - {file_ext}"
                )]

        except Exception as e:
            return [types.TextContent(
                type="text",
                text=f"è¯»å–æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}"
            )]

    else:
        return [types.TextContent(
            type="text",
            text=f"æœªçŸ¥å·¥å…·: {name}"
        )]

# å°è¯•å¯¼å…¥NotificationOptionsï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ
try:
    from mcp.server.lowlevel.server import NotificationOptions
    notification_options = NotificationOptions()
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨Noneæˆ–ç©ºå­—å…¸
    notification_options = None

async def main():
    """ä½¿ç”¨æ ‡å‡†è¾“å…¥è¾“å‡ºè¿è¡ŒæœåŠ¡å™¨"""
    try:
        async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
            # æ„å»ºcapabilitieså‚æ•°
            capabilities_kwargs = {"experimental_capabilities": {}}
            if notification_options is not None:
                capabilities_kwargs["notification_options"] = notification_options

            await server.run(
                read_stream,
                write_stream,
                InitializationOptions(
                    server_name="document-reader",
                    server_version="1.1.0",
                    capabilities=server.get_capabilities(**capabilities_kwargs),
                ),
            )
    except Exception as e:
        print(f"æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())